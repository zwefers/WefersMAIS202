# -*- coding: utf-8 -*-
"""Artist Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YZro8Rf2mC0Jzj6al6QBpcVzXLByoevM
"""

# getting data from google drive
from google.colab import drive

drive.mount('/content/gdrive')

import glob
import matplotlib.pyplot as plt
from IPython.display import display, Image

import cv2
import numpy as np


folder_list = ['Degas', 'Durer', 'Gaugin', 'Goya', 'Picasso', 'Renoir', 'Sisley', 'Titian', 'Rembrandt', 'Van Gogh' ]
train_data = []
test_data = []
paths = []
for folder in folder_list:
  paths = list(glob.glob('gdrive/My Drive/MAIS Final Project Data/{}/*.jpg'.format(folder)))
  print('gdrive/My Drive/MAIS Final Project Data/{}/*.jpg'.format(folder))
  i = 0
  for img in paths:  
    # read in picture at path
    img = cv2.imread(img)
    if i < 30:
      test_data.append([img, folder, [0]])
    else:
      train_data.append([img, folder, [0]])
    i = i + 1

print(np.asarray(train_data).shape)
train_data_copy = np.asarray(train_data).copy()
train_data = np.asarray(train_data)
np.random.shuffle(train_data)

# Commented out IPython magic to ensure Python compatibility.
import os, sys, math
import numpy as np
import cv2
from matplotlib import pyplot as plt
if 'google.colab' in sys.modules: # Colab-only Tensorflow version selector
#   %tensorflow_version 2.x
import tensorflow as tf
print("Tensorflow version " + tf.__version__)
AUTO = tf.data.experimental.AUTOTUNE

import PIL
from PIL import Image
for data in train_data:
  display(PIL.Image.fromarray(data[0]))
  print(data[1])

# Detect hardware
try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection
except ValueError:
  tpu = None
  gpus = tf.config.experimental.list_logical_devices("GPU")
    
# Select appropriate distribution strategy for hardware
if tpu:
  tf.config.experimental_connect_to_cluster(tpu)
  tf.tpu.experimental.initialize_tpu_system(tpu)
  strategy = tf.distribute.experimental.TPUStrategy(tpu)
  print('Running on TPU ', tpu.master())  
elif len(gpus) > 0:
  strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs
  print('Running on ', len(gpus), ' GPU(s) ')
else:
  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU
  print('Running on CPU')

# How many accelerators do we have ?
print("Number of accelerators: ", strategy.num_replicas_in_sync)

import keras
max1 = 0
max2 = 0
i = 0
k = 0
l = 0
for data in train_data:
  if (np.asarray(train_data[i][0]).shape[0] > max1):
    max1 = np.asarray(train_data[i][0]).shape[0]
    k = i
  if (np.asarray(train_data[i][0]).shape[1] > max2):
    max2 = np.asarray(train_data[i][0]).shape[1]
    l = i
  i = i+1
print(max1)
print(k)
print(max2)
print(l)
print(np.asarray(train_data[1667][0]).shape)
print(np.asarray(train_data[486][0]).shape)

print(np.asarray(train_data).shape)
print(np.asarray(test_data).shape)

from keras.utils import to_categorical
from PIL import Image

x = 1
y = 1
scale = 0.0
train_labels = [None] * 3760
train_set = [None] * 3760
i = 0
for data in train_data:
  img = np.asarray(data[0])
  x = np.asarray(img).shape[0]
  y = np.asarray(img).shape[1]
  if x < y:
    y = int(400/x * y)
    x = 400
    img = cv2.resize(img, (y, x))
    train_set[i] = img[: , int((y-400)/2) : 400 + int((y-400)/2), :]
  else:
    x = int(400/y * x)
    y = 400
    img = cv2.resize(data[0], (y, x))
    train_set[i] = img[int((x-400)/2) : 400 + int((x-400)/2), :, :]
  if data[1] == "Degas":
    train_labels[i] = to_categorical(0, num_classes=10)
  elif data[1] == "Durer":
    train_labels[i] = to_categorical(1, num_classes=10)
  elif data[1] == "Gaugin":
    train_labels[i] = to_categorical(2, num_classes=10)
  elif data[1] == "Goya":
    train_labels[i] = to_categorical(3, num_classes=10)
  elif data[1] == "Picasso":
    train_labels[i] = to_categorical(4, num_classes=10)
  elif data[1] == "Renoir":
    train_labels[i] = to_categorical(5, num_classes=10)
  elif data[1] == "Sisley":
    train_labels[i] = to_categorical(6, num_classes=10)
  elif data[1] == "Titian":
    train_labels[i] = to_categorical(7, num_classes=10)
  elif data[1] == "Rembrandt":
    train_labels[i] = to_categorical(8, num_classes=10)
  elif data[1] == "Van Gogh":
    train_labels[i] = to_categorical(9, num_classes=10)
  i = i +1

from keras.utils import to_categorical
from PIL import Image

x = 1
y = 1
scale = 0.0
test_labels = [None] * 300
test_set = [None] * 300
i = 0
for data in test_data:
  img = np.asarray(data[0])
  x = np.asarray(img).shape[0]
  y = np.asarray(img).shape[1]
  if x < y:
    y = int(400/x * y)
    x = 400
    img = cv2.resize(img, (y, x))
    test_set[i] = img[: , int((y-400)/2) : 400 + int((y-400)/2), :]
  else:
    x = int(400/y * x)
    y = 400
    img = cv2.resize(data[0], (y, x))
    test_set[i] = img[int((x-400)/2) : 400 + int((x-400)/2), :, :]
  if data[1] == "Degas":
    test_labels[i] = to_categorical(0, num_classes=10)
  elif data[1] == "Durer":
    test_labels[i] = to_categorical(1, num_classes=10)
  elif data[1] == "Gaugin":
    test_labels[i] = to_categorical(2, num_classes=10)
  elif data[1] == "Goya":
    test_labels[i] = to_categorical(3, num_classes=10)
  elif data[1] == "Picasso":
    test_labels[i] = to_categorical(4, num_classes=10)
  elif data[1] == "Renoir":
    test_labels[i] = to_categorical(5, num_classes=10)
  elif data[1] == "Sisley":
    test_labels[i] = to_categorical(6, num_classes=10)
  elif data[1] == "Titian":
    test_labels[i] = to_categorical(7, num_classes=10)
  elif data[1] == "Rembrandt":
    test_labels[i] = to_categorical(8, num_classes=10)
  elif data[1] == "Van Gogh":
    test_labels[i] = to_categorical(9, num_classes=10)
  i = i +1

from PIL import Image
print(train_data[1200][1])
print(train_labels[1200])
#display(Image.fromarray(train_set[1400]))

train_labels = np.asarray(train_labels)
train_set = np.asarray(train_set)

train_labels = np.vstack(train_labels)
print(train_labels.shape)

from sklearn.utils import class_weight

y_train = [np.argmax(label) for label in train_labels]
 
class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)

#Optimizers

opt1 = tf.keras.optimizers.SGD(lr=0.001, momentum=0.90)
opt2 = tf.keras.optimizers.Adam(lr=0.001)
opt3 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.95)
opt4 = tf.keras.optimizers.Adagrad(learning_rate=0.001)
opt5 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)

with strategy.scope(): # Tensorflow wants us to declare and compile our model within this "scope".
  # pretrained_model = tf.keras.applications.Xception(input_shape=[*IMAGE_SIZE, 3],  include_top=False)
  pretrained_model = tf.keras.applications.VGG19(weights=None, include_top=False, input_shape=[400, 400, 3])
  

  x = pretrained_model.output
  x = tf.keras.layers.GlobalAveragePooling2D()(x)
  x = tf.keras.layers.Dense(1024, activation='relu')(x)
  x = tf.keras.layers.Dropout(0.2)(x)
  # Only 10 categories.
  predictions = tf.keras.layers.Dense(10, activation='softmax')(x)

  model = tf.keras.Model(inputs=pretrained_model.input, outputs=predictions) 
  model.compile(optimizer=opt5, loss='categorical_crossentropy', metrics=['accuracy'])
  model.summary()

history = model.fit(x=train_set, y=train_labels, batch_size=18, epochs=5, validation_split=0.1, class_weight=class_weights)

pred = model.predict(test_set)

test_set = np.asarray(test_set)

print(test_set.shape)
pred = model.predict(test_set)

#saving model
model.save("model.h5")

#saving weights
model.save_weights("weights.h5")

print(pred.shape)
count = 0;
for i in range(300):
  if np.argmax(pred[i]) == np.argmax(test_labels[i]):
    count +=1
print(count)

#making np arrays to make confusion matrix
print(pred.shape)
pred_labels = [None] * 300 
pred2_labels = [None] * 300 
labels = [None] * 300 
for i in range(300):
  pred_labels[i] = np.argmax(pred[i])
  labels[i] = np.argmax(test_labels[i])
  i = i + 1
pred_labels = np.asarray(pred_labels)
labels = np.asarray(labels)
print(pred_labels.shape)
print(labels.shape)

for i in range(300):
  pred_labels[i] = np.argmax(pred[i])
  labels[i] = np.argmax(test_labels[i])
  i = i + 1

#making confusion matrix
con_matrix = tf.math.confusion_matrix(labels=labels, predictions=pred_labels).numpy()

print(con_matrix)

# list all data in history
print(history.history.keys())

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# baseline model

baseline =  tf.keras.Sequential()
baseline.add(tf.keras.layers.Flatten())
baseline.add(tf.keras.layers.Dense(32, input_dim=48000))
baseline.add(tf.keras.layers.Activation('relu'))
baseline.add(tf.keras.layers.Dense(10, activation='softmax'))



baseline.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

baseline.summary()

history = baseline.fit(x=train_set, y=train_labels, batch_size=18, epochs=10, validation_split=0.1)

base_pred = baseline.predict(test_set)

print(base_pred.shape)
count = 0;
for i in range(300):
  if np.argmax(base_pred[i]) == np.argmax(test_labels[i]):
    count +=1
print(count)